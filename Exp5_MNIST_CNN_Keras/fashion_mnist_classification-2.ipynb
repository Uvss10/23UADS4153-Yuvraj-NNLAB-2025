{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YqSluS9G02_"
      },
      "source": [
        "<h1>Experiment 5</h1>\n",
        "\n",
        "### Objective\n",
        "<h4> WAP  to  train  and  evaluate  a  convolutional  neural  network  using  Keras  Library  to\n",
        "classify  MNIST  fashion  dataset.  Demonstrate  the  effect  of  filter  size,  regularization,\n",
        "batch size and optimization algorithm on model performance. </h4>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXu_qJH7G03C"
      },
      "source": [
        "This project implements a **Convolutional Neural Network (CNN)** using TensorFlow and Keras to classify images from the **Fashion MNIST** dataset.\n",
        "\n",
        "\n",
        "\n",
        "### Dataset\n",
        "- **Dataset:** Fashion MNIST (28x28 grayscale images across 10 classes)\n",
        "- **Preprocessing:**\n",
        "  - Normalized pixel values to [0, 1]\n",
        "  - Reshaped to (28, 28, 1)\n",
        "  - One-hot encoded the labels\n",
        "  - Used 'ImageDataGenerator' for data augmentation:\n",
        "\n",
        "###  Model Architecture\n",
        "\n",
        "- **Input Layer:** (28, 28, 1)\n",
        "- **Conv Block 1:**\n",
        "  - 2 × Conv2D → BatchNorm → ReLU\n",
        "  - MaxPooling2D\n",
        "  - Dropout\n",
        "- **Conv Block 2 (Residual Block):**\n",
        "  - 2 × Conv2D → BatchNorm → Add(Shortcut) → ReLU\n",
        "  - MaxPooling2D\n",
        "  - Dropout\n",
        "- **Conv Block 3:**\n",
        "  - Conv2D\n",
        "  - GlobalAveragePooling2D\n",
        "- **Dense Layers:**\n",
        "  - Dense(512) → BatchNorm → ReLU → Dropout\n",
        "  - Output: Dense(10) with Softmax\n",
        "\n",
        "\n",
        "### Training Hyperparameters\n",
        "\n",
        "- **Optimizer:** 'Adam' with Exponential Learning Rate Decay\n",
        "- **Loss Function:** 'CategoricalCrossentropy' with 'label_smoothing=0.1'\n",
        "\n",
        "### Evaluation\n",
        "\n",
        "- Loads the best saved model weights using 'ModelCheckpoint'.\n",
        "- Evaluates on the test set\n",
        "- Plots training and validation loss and accuracy over epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqY5S0R8G03D",
        "outputId": "18022527-595a-4574-da6f-ff09a20ceb0d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 57ms/step - accuracy: 0.6542 - loss: 1.4884 - val_accuracy: 0.8234 - val_loss: 0.6053\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 55ms/step - accuracy: 0.8070 - loss: 0.6516 - val_accuracy: 0.8506 - val_loss: 0.5346\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 59ms/step - accuracy: 0.8297 - loss: 0.5865 - val_accuracy: 0.8658 - val_loss: 0.4886\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 55ms/step - accuracy: 0.8413 - loss: 0.5528 - val_accuracy: 0.8715 - val_loss: 0.4624\n",
            "Epoch 5/10\n"
          ]
        }
      ],
      "source": [
        "# Python Implementation\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build the CNN model\n",
        "def build_model(filter_size=3, regularization=None, optimizer='adam'):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolutional layers\n",
        "    model.add(layers.Conv2D(32, (filter_size, filter_size), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (filter_size, filter_size), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Flatten and Dense layers\n",
        "    model.add(layers.Flatten())\n",
        "    if regularization:\n",
        "        model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularization))\n",
        "    else:\n",
        "        model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Experiment with different hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "\n",
        "# Example: Using filter size 3, L2 regularization, and Adam optimizer\n",
        "model = build_model(filter_size=3, regularization=regularizers.l2(0.01), optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curve')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Code Description</h2>\n",
        "Dataset<br>\n",
        "Fashion MNIST: Comprises 28x28 grayscale images representing 10 different clothing categories.<br>\n",
        "Classes: 10 fashion categories (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot).<br>\n",
        "Preprocessing:<br>\n",
        "Pixel values are normalized to the range [0, 1] by dividing by 255.<br>\n",
        "Data is reshaped from (28, 28) to (28, 28, 1) for CNN compatibility.<br>\n",
        "Labels are one-hot encoded using to categorical.<br>\n",
        "<br>\n",
        "\n",
        "Model Architecture<br>\n",
        "Input Layer: Accepts input images of shape (28, 28, 1).<br>\n",
        "Convolutional Layers:<br>\n",
        "Conv Layer 1: 32 filters of size (3x3), ReLU activation.<br>\n",
        "MaxPooling: (2x2) pooling to reduce spatial dimensions.<br>\n",
        "Conv Layer 2: 64 filters of size (3x3), ReLU activation.<br>\n",
        "MaxPooling: (2x2) pooling again for spatial reduction.<br>\n",
        "Fully Connected Layers:<br>\n",
        "Flatten layer to convert 2D features to 1D.<br>\n",
        "Dense(128) layer with ReLU activation. Optional L2 Regularization can be applied via kernel_regularizer=regularizers.l2(0.01).<br>\n",
        "Dropout(0.5) to prevent overfitting.<br>\n",
        "Final Dense(10) layer with Softmax activation for output classification.<br>\n",
        "\n",
        "\n",
        "Training Strategy<br>\n",
        "Optimizer: Adam optimizer is used for efficient training.<br>\n",
        "Loss Function: Categorical Crossentropy (suitable for multi-class classification).<br>\n",
        "Metrics: Model performance is tracked using accuracy.<br>\n",
        "\n",
        "\n",
        "Training<br>\n",
        "Model is trained on preprocessed training data.<br>\n",
        "A validation split of 20% is used to monitor generalization.<br>\n",
        "Training is performed for 10 epochs with a batch size of 64.<br>\n",
        "Training and validation accuracy/loss curves are plotted for performance visualization.<br>\n",
        "\n",
        "\n",
        "Evaluation<br>\n",
        "Model performance is evaluated on the test dataset.<br>\n",
        "Final Test Accuracy is printed.<br>\n",
        "Accuracy and loss trends over epochs are visualized using matplotlib.<br>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5FuLLv5lPi31"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CyatSmDG03E"
      },
      "source": [
        "### Performance Evaluation\n",
        "- **Accuracy**: The test accuracy achieved is printed after evaluation.\n",
        "- **Loss Curve**: The training and validation loss curves are plotted to visualize convergence.\n",
        "- **Accuracy Curve**: The training and validation accuracy curves are plotted to observe learning progress.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMMdbo_WG03E"
      },
      "source": [
        "<h2>My Comments :-</h2>\n",
        "\n",
        "<ul>\n",
        "<li>The maximum test accuracy achieved is 88.53.</li><br>\n",
        "<li>Adding the Batch Normalisation and dropout layers along with applying the data augmentation proved effective in bringing around 95% accuracy.</li><br>\n",
        "<li>Additionally in the previous model the gap between the validation and training set was prominent indicating model overtraining which is reduced to a significant extent in this model eventually fixing the problem of model overtraining.</li>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tfenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}