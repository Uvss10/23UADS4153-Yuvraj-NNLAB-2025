** Objective ** :- 
To implement a multi-layer perceptron (MLP) with one hidden layer using NumPy and demonstrate that it can correctly learn
the XOR Boolean function.

** Description of the Model **
For this experiment,an MLP is designed with one hidden layer to solve the XOR function, which is a non-linearly separable problem. Since a single-layer perceptron cannot solve XOR, there is need to introduce hidden layer with multiple neurons .

Model Components

Input Layer: 2 neurons (for two input bits)

Hidden Layer: 4 perceptrons (to learn intermediate patterns)

Activation Function: Step function (binary output)

Output Layer**: 1 perceptron (final XOR output)

** Description of Code ** :- 
1.   Perceptron Class:

        *   Implements a simple perceptron with step activation function.
        *   Performs weight updates using the perceptron learning rule.


2.   Hidden Layer Training:

        *   Trains 4 separate perceptrons to learn intermediate patterns for XOR.


3.  Final Output Layer Training:

      *   Uses the outputs from the hidden layer as inputs to a final perceptron.
      *   This final perceptron learns the XOR function.


** Performance Evaluation ** :- 

XOR Truth Table Predictions:
 X1  X2 |  y_actual  y_pred
---------------------------
 0   0  |     0        0
 0   1  |     1        1
 1   0  |     1        1
 1   1  |     0        0
Predicted output for [0 0]: 0
Final Perceptron Accuracy: 100.00%

2025-02-23 22:20:20.273 python[9000:215364] +[IMKClient subclass]: chose IMKClient_Modern
2025-02-23 22:20:20.273 python[9000:215364] +[IMKInputSession subclass]: chose IMKInputSession_Modern

** Accuracy ** :
- 100% accuracy shows that the perceptron has perfectly learned and classified the XOR logic gate.

** Confusion Matrix **:
- Shows correct classifications for XOR, provding MLP learns nonlinear functions.

** Comment **
- ** Limitations: **
  * A manually designed 4-perceptron hidden layer works for XOR but may not generalize well to more complex problems.
  * Training with a manually defined structure takes longer than using a standard multi-layer perceptron with backpropagation.
  * The use of `tanh` activation provides smoother updates but may still suffer from vanishing gradients in deeper networks.

- ** Scope for Improvement: **
  * Implementing a full **feedforward neural network with backpropagation** can automatically learn the hidden layer without manual tuning.
  * Using a Sigmoid or ReLU activation function instead of `tanh` can improve gradient flow and training efficiency.
  * Applying **batch training** instead of updating weights after each sample can stabilize training.


  




